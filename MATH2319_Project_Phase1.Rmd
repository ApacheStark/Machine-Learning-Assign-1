---
title: "MATH2319_Assignment_Part1"
author: "Samuel Holt (s3381728) and Margaret Cuddihy (s3608125)"
date: "28 April 2019"
output: 
   html_document:
      toc: true
      toc_depth: 3
      prettydoc::html_pretty:
      theme: tactile
      
---




# Introduction

The data set chosen for machine learning training is Google advertising data. 

The data set was made available on Kaggle as part of the MATH2319 competition and was accessed from the following link:
https://www.kaggle.com/c/machine-learning-battle-mlmath2319/data

#### Goal

By training a machine learning algorithm to the descriptive features in the data set, the target feature, a revenue metric labelled as y, can be predicted. Hence, the purpose of training a machine learning algorithm to this data set is that Google will be able to better predict the advertising specifications that optimise returns on clicks and potentially improve revenue returns. 

For this first phase of the project, the objective is to inspect the data items in the data set in order to gain an understanding of each feature's behaviour and to highlight any significant relationships between features. This understanding will guide the strategy of the second phase of the project where the machine learning algorithm is designed and tested. This initial inspection will also aim to identfy any data quality issues that will be appropriately handled before machine learning modelling begins. 

#### Description of the Data Set
The dataset instances are comprised of website traffic records from around the world. Descriptive features include:

* companyId: - Company ID of record (categorical)

*	countryId: - Country ID of record (categorical)

*	deviceType: - Device type of record (categorical corresponding to desktop, mobile, tablet)

*	day: -Day of record (integer between 1 (oldest) and 30 for train, 31 and 35 (most recent) for test)

*	dow: - Day of week of the record (categorical)

*	price1, price2, price3: - Price combination for the record set by the company (numeric)

*	ad_area: - area of advertisement (normalized between 0 and 1)

*	ad_ratio: - ratio of advertisement's length to its width (normalized between 0 and 1)

*	requests, impression, cpc, ctr, viewability: - Various metrics related to the record (numeric)

*	ratio1 - ratio5: - Ratio characteristics related to the record (each normalized between 0 and 1)

*	y (target feature): - revenue-related metric (numeric)

The currency features are in US Dollars.

#### Structure of Phase 1
First, the data will be inspected, and any necessary pre-processing will be undertaken to ensure the data set is clean and ready for modelling.

1.	Data structure and dimensions will be determined
2.	Data will be checked for any missing or special values which will be appropriately imputed
3.	Data will be checked for outliers which will be handled as necessary 

Second, the descriptive features of the data will be explored in order to identify significant relationships between features that will guide the machine learning modelling. For instance, descriptive features with strong correlation may be redundant to predictive modelling.

1.	Distribution of descriptive feature levels will be examined
2.	Correlation between descriptive features will be calculated
3.	Correlation between descriptive features and the target feature will be calculated

The aim of this is to identify the descriptive features most useful for predicting the target variable, the revenue metric y.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
```{r}
 library(dplyr)
# library(tidyr)
 library(knitr)
# library(mlr)
# library(outliers)
library(ggplot2)
library(Hmisc)
library(corrplot)

```
# Reading and Inspecting the Data
```{r echo=TRUE}

advertising_train <- read.csv("~/Uni/Machine Learning/advertising_train.csv", header = TRUE, stringsAsFactors = FALSE)

head(advertising_train)
tail(advertising_train)
dim(advertising_train)
str(advertising_train)
names(advertising_train)
class(advertising_train)
```

## Target Feature
y: numeric revenue-related metric

## Unique Identifier
Case ID: categorical

## Descriptive Features
1. Company ID: categorical 
2. Country ID: categorical
3. Device Type: categorical (Unique values = 1,2,3 and 5)
4. Day: integer (Day 1 to Day 30 in training data)
5. DOW: categorical (day of the week)
6. Price 1 (numeric)
7. Price 2 (numeric)
8. Price 3 (numeric)
9. Ad Area (normalised between 1 and 0)
10. Ad Ratio (normalised between 1 and 0)
11. Requests (numeric)
12. Impression (numeric)
13. CPC (numeric)
14. CTR (numeric)
15. Viewability (numeric)
16. Ratio 1 (normalised between 1 and 0)
17. Ratio 2 (normalised between 1 and 0)
18. Ratio 3 (normalised between 1 and 0)
19. Ratio 4 (normalised between 1 and 0)
20. Ratio 5 (normalised between 1 and 0)

214,218 Instances

# Cleaning and Preparing Data

```{r echo=TRUE}
kable(summary(advertising_train))

```
## Transformation and Pre Processing

### Factorisation of Categorical Variables
```{r}
# case_id can be filtered
# company id
# country id
# deviceType
# day
for (i in 1:5) advertising_train[,i] <- factor(advertising_train[,i])
```

### Log Transform
The following function will log transform all integer and numeric data, also considers zero or negative values in the scale and adds a scalar value to the column before transformation if this parameter is met.
```{r}
# Make a copy of non-transformed
train <- advertising_train
# Create log trans function
log_trans <- function(x) {
  for (i in 1:ncol(x))  
  if (class(x[,i]) == 'integer' | class(x[,i]) == 'numeric') {
    if (min(x[,i]) <= 0) {
      x[,i] <- x[,i] + min(x[,i]) + 0.1
      x[,i] <- log(x[,i])
    } else {
    x[,i] <- log(x[,i])
    } 
  }
  return(x)
}
train_log <- log_trans(advertising_train)
```

## Special Values

### Missing
```{r}
which(is.na(advertising_train)) 

```
No missing values detected. 


### Special
```{r include=FALSE}
# Check for special values
is.special <- function(x){
  if (is.numeric(x)) !is.finite(x)
}

sapply(advertising_train, is.special)

```
A function was created to check for special values. It has been suppressed for brevity. No special values detected.  


## Outliers

### Univariate

```{r}
#Categorical Features 

#--------------------[Company]----------------------#

company <- advertising_train %>% 
    group_by(companyId) %>% 
    dplyr::summarise(count=n())

company %>% 
    ggplot(aes(x = companyId, y = count)) +
    geom_col()
```
Company:  
The majority of instances belong to Company ID 43. 

```{r}

#--------------------[Country]----------------------#

country <- advertising_train %>% 
    group_by(countryId) %>% 
    dplyr::summarise(count=n())

country %>% 
    ggplot(aes(x = countryId, y = count)) +
    geom_col()


```
Country ID:  
We do not have access to the Country ID concordance to country names

```{r}
#--------------------[Device Type]----------------------#

device <- advertising_train %>% 
    group_by(deviceType) %>% 
    dplyr::summarise(count=n())

device %>% 
    ggplot(aes(x = deviceType, y = count)) +
    geom_col()

```
Device Type:  
Majority level of device type is 2. 

```{r}
#--------------------[Day of Week]----------------------#

advertising_train$dow <- factor(advertising_train$dow, levels = c('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'))

day.of.week <- advertising_train %>% 
    group_by(dow) %>% 
    dplyr::summarise(count=n())



day.of.week %>% 
    ggplot(aes(x = dow, y = count)) +
    geom_col()


```
Day of Week:  
There is a higher level of instances on Saturday and Sunday. 

```{r}
#--------------------[Day]----------------------#

day <- advertising_train %>% 
    group_by(day) %>% 
    dplyr::summarise(count=n())

day %>% 
    ggplot(aes(x = day, y = count)) +
    geom_col()


```
#Numeric Features
```{r}
#--------------------[Price 1]----------------------#


price1 <- train$price1
summary(price1)

train %>% 
    group_by(price1) %>% 
    dplyr::summarise(count=n())


price1_log <- train_log$price1
summary(price1_log)

train_log%>% 
    group_by(price1) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(price1)
boxplot(price1)

hist(price1_log)
boxplot(price1_log)
```
```{r}
#--------------------[Price 2]----------------------#


price2 <- train$price2
summary(price2)

train %>% 
    group_by(price2) %>% 
    dplyr::summarise(count=n())


price2_log <- train_log$price2
summary(price2_log)

train_log%>% 
    group_by(price2) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(price2)
boxplot(price2)

hist(price2_log)
boxplot(price2_log)
```
```{r}
#--------------------[Price 3]----------------------#

price3 <- train$price3
summary(price3)

train %>% 
    group_by(price3) %>% 
    dplyr::summarise(count=n())


price3_log <- train_log$price3
summary(price3_log)

train_log%>% 
    group_by(price3) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(price3)
boxplot(price3)

hist(price3_log)
boxplot(price3_log)
```

Price Features:  
Majority of values are zero and the distribution is heavily positively skewed. 
The log transformation normalises the distibution slightly, but as it does not offer much improvement, we will use the original data. 


```{r}
#--------------------[Ad Area]----------------------#


ad_area <- train$ad_area
summary(ad_area)

train %>% 
    group_by(ad_area) %>% 
    dplyr::summarise(count=n())


ad_area_log <- train_log$ad_area
summary(ad_area_log)

train_log%>% 
    group_by(ad_area) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(ad_area)
boxplot(ad_area)

hist(ad_area_log)
boxplot(ad_area_log)


```
```{r}
#--------------------[Ad Ratio]----------------------#


ad_ratio <- train$ad_ratio
summary(ad_ratio)

train %>% 
    group_by(ad_ratio) %>% 
    dplyr::summarise(count=n())


ad_ratio_log <- train_log$ad_ratio
summary(ad_ratio_log)

train_log%>% 
    group_by(ad_ratio) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(ad_ratio)
boxplot(ad_ratio)

hist(ad_ratio_log)
boxplot(ad_ratio_log)


```
Ad Area: 
The description of this feature states that it has been normalised between 0 and 1. The majority of values are extremely small (0.001) or over 1, which is not consistent with this description. However, without more information regarding the data, we are reluctant to adjust the data further we are unsure if there is an error in the description or the normalised calculation. This will be further investigated in Phase 2 and adjustments will be made if necessary. 

Ad Ratio:   
The description of this feature stats that it has been normalised between 0 and 1. In comparison to Ad Area, the distribution does appear to largely fall within 0 and 1, with a median value of 1. There are a small number of instances with values greater than 1. Similar to Ad Area, without further information regarding the data, it is difficult to say with certainty that these are outliers so we have decided not to constrain the data using winsorisation. This will be further evaluated during modelling and adjusted if necessary. 

The log transformation does not do much to normalise the distribution of the data.  

```{r}
#--------------------[Requests]----------------------#



requests <- train$requests
summary(requests)

train %>% 
    group_by(requests) %>% 
    dplyr::summarise(count=n())


requests_log <- train_log$requests
summary(requests_log)

train_log%>% 
    group_by(requests) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(requests)
boxplot(requests)

hist(requests_log)
boxplot(requests_log)

```
```{r}
#--------------------[Impression]----------------------#



impression <- train$impression
summary(impression)

train %>% 
    group_by(impression) %>% 
    dplyr::summarise(count=n())


impression_log <- train_log$impression
summary(impression_log)

train_log%>% 
    group_by(impression) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(impression)
boxplot(impression)

hist(impression_log)
boxplot(impression_log)
```
Requests and Impression:  
These features are very similar, with the majority of instances taking the value of zero. The data is spread across a very long range, with the maximum value of Requests equal to 6,701,924 and the maximum value of Impression equal to 6,100,324.

The log transformation improves the distribution of the data. 

```{r}
#--------------------[CPC - Cost per click]----------------------#

cpc <- train$cpc
summary(cpc)

train %>% 
    group_by(cpc) %>% 
    dplyr::summarise(count=n())


cpc_log <- train_log$cpc
summary(cpc_log)

train_log%>% 
    group_by(cpc) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(cpc)
boxplot(cpc)

hist(cpc_log)
boxplot(cpc_log)
```
```{r}
#--------------------[CTR - click through rate]----------------------#


ctr <- train$ctr
summary(ctr)

train %>% 
    group_by(ctr) %>% 
    dplyr::summarise(count=n())


ctr_log <- train_log$ctr
summary(ctr_log)

train_log%>% 
    group_by(ctr) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(ctr)
boxplot(ctr)

hist(ctr_log)
boxplot(ctr_log)

```
Cost Per Click (CPC) and Click Through Rate (CTR):
Majority of values are zero and the log transformation does not do much to improve the skewedness of the data. Additional analysis is conducted below in order to disaggregate the zero value instances from the rest. 

```{r}
#--------------------[Viewability]----------------------#

viewability <- train$viewability
summary(viewability)

train %>% 
    group_by(viewability) %>% 
    dplyr::summarise(count=n())


viewability_log <- train_log$viewability
summary(viewability_log)

train_log%>% 
    group_by(viewability) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(viewability)
boxplot(viewability)

hist(viewability_log)
boxplot(viewability_log)

```
Viewability: 
Similar to the other advertisement metric data, the majority of instances take on the value of zero. The log transformation improves the skewedness of the data. 


```{r}
#--------------------[Ratio 1]----------------------#

summary(advertising_train$ratio1)

advertising_train %>% 
    group_by(ratio1) %>% 
    dplyr::summarise(count=n())

hist(advertising_train$ratio1)
boxplot(advertising_train$ratio1)
```
```{r}
#--------------------[Ratio 2]----------------------#

summary(advertising_train$ratio2)

advertising_train %>% 
    group_by(ratio2) %>% 
    dplyr::summarise(count=n())

hist(advertising_train$ratio2)
boxplot(advertising_train$ratio2)
```
```{r}
#--------------------[Ratio 3]----------------------#

summary(advertising_train$ratio3)

advertising_train %>% 
    group_by(ratio3) %>% 
    dplyr::summarise(count=n())

hist(advertising_train$ratio3)
boxplot(advertising_train$ratio3)
```
```{r}
#--------------------[Ratio 4]----------------------#

summary(advertising_train$ratio4)

advertising_train %>% 
    group_by(ratio4) %>% 
    dplyr::summarise(count=n())

hist(advertising_train$ratio4)
boxplot(advertising_train$ratio4)
```
```{r}
#--------------------[Ratio 5]----------------------#

summary(advertising_train$ratio5)

advertising_train %>% 
    group_by(ratio5) %>% 
    dplyr::summarise(count=n())

hist(advertising_train$ratio5)
boxplot(advertising_train$ratio5)


```
Ratio Features: 
We pressume that these ratio features are calculated using the other advertisement metrics data. As such, it is no surprise to see a large amount of zero values. However, the normalising nature of the ratios nicely constrains the data values between 0 and 1. We have chosen not to use the log transformation on the Ratio features as they are constrained and not as skewed as some of the other metrics. Since we do not have information on how these ratios are calculated, our understanding of their use will come from examining correlation between ratios and other features later in the report. 

```{r}
#--------------------[Target Variable (y)]----------------------#

y <- train$y
summary(y)

train %>% 
    group_by(y) %>% 
    dplyr::summarise(count=n())


y_log <- train_log$y
summary(y_log)

train_log%>% 
    group_by(y) %>% 
    dplyr::summarise(count=n())

par(mfrow = c(1,4))
hist(y)
boxplot(y)

hist(y_log)
boxplot(y_log)


```
Target Feature (y):  
The majority of values fall below 1, while the maximum value is 47, resulting in highly positively skewed data. By applying a log transformation to the target feature, the distribution is greatly improved in terms of normality. For creating a machine learning model, we will use log transformed values of y. This means that once the model is determined and predictions are calculated, they will have to be returned from log values to original values. 


### Multivariate 
```{r}
par(mfrow = c(1,1))

y <- advertising_train$y

boxplot(y~advertising_train$companyId)

plot(y~advertising_train$countryId)

boxplot(y~advertising_train$deviceType)

plot(y~advertising_train$day)

boxplot(y~advertising_train$dow)

plot(y~advertising_train$price1)

plot(y~advertising_train$price2)

plot(y~advertising_train$price3)

plot(y~advertising_train$ad_ratio)

plot(y~advertising_train$ad_area)

plot(y~advertising_train$requests)

plot(y~advertising_train$impression)

plot(y~advertising_train$cpc)

plot(y~advertising_train$ctr)

plot(y~advertising_train$viewability)

plot(y~advertising_train$requests)

plot(y~advertising_train$ratio1)

plot(y~advertising_train$ratio2)

plot(y~advertising_train$ratio3)

plot(y~advertising_train$ratio4)

plot(y~advertising_train$ratio5)


```


# Visualisation

## Bivariate Visuals with Respect to Target Feature

### Prices
```{r}

#Calculate Average Price and Maximum Price
train1 <- train %>% mutate(avg_price = (price1*price2*price3)/3,
                           max_price = pmax(price1, price2, price3))

summary(train1$avg_price)
summary(train1$max_price)

par(mfrow = c(3,1))
plot(train1$avg_price, train1$y)
plot(train1$max_price, train1$y)
par(mfrow = c(1,1))

# Price Data Comparison
par(mfrow = c(3,1))
plot(train$price1, train$y) # all pos skewed, majority at lower quartile of range, trace values at upper 3 quartiles
plot(train$price2, train$y)
plot(train$price3, train$y)
par(mfrow = c(1,1))

```

Note that the maximum price has the same distribution against the dependent variable y as Price 3. Therefore, this aggregate is redundant as it does not provide any additional value in predicting y. Average price may be considered if it shows to be reasonably useful at helping to predict y - a single descriptive feature would be preferable to three separate price features if both provide a similar level of usefulness in predicting y. This will be explored in Phase 2 of the project. 

### Ratios
```{r}
# Ratio Data Comparison
par(mfrow = c(1,5))
plot(train$ratio1, train$y) # majority of variation at 0 and 1 ratios 
plot(train$ratio2, train$y) # majority of variation at 0 and 1 ratios 
plot(train$ratio3, train$y) # majority of variation at 0 and 1 ratios, with some exceeding 1 
plot(train$ratio4, train$y) # majority of variation at 0 
plot(train$ratio5, train$y) # majority of variation at 0 and some at 1 ratios, with some exceeding 1
par(mfrow = c(1,1))
```

### CPC and CTR
Cost Per Click (also known as PPC, Pay Per Click) and Click Through Rate will be investigated for impact on each other and on the target variable.
```{r}
# COST PER CLICKS
# This is the advertising cost per click for given ad, as price goes up, the 'rank' of the ad decreases exponentially.
# Essentially, you want to minimise cost per click, which reflects your score, visa versa.
cpc_id_box <- boxplot(cpc ~ companyId, data = train)
cpc_filt <- train %>% select(companyId, cpc) %>%  filter(cpc > 0 & cpc < 1)
cpc_filt_box <- boxplot(cpc ~ companyId, data = cpc_filt) # company makes a difference
cpc_zero <- train %>% select(companyId, cpc) %>%  group_by(companyId) %>% filter(cpc == 0) %>%  summarise(ZeroVal = n())
cpc_more <- train %>% select(companyId, cpc) %>%  group_by(companyId) %>%  summarise(Total = n())
cpc_tbl <- cbind(cpc_zero, cpc_more[2]) %>%  as.tbl %>%  mutate(Ratio = ZeroVal / Total,
                                                                NonZero = Total - ZeroVal)
# RATIO OF ZERO CPC TO TOTAL
ggplot(data = cpc_tbl,
       mapping = aes(y = Ratio, x = companyId, fill = companyId)) +
  geom_bar(stat = 'identity', position = 'dodge') + 
  labs(title = 'Ratio of Zero Values against Total')
```
```{r}
# CLICK THROUGH RATE
# Perecentage of viewers on a web page who selected a hyperlink to a site
# CTR = (Total Measured Clicks/Total Measured Ad Impressions) * 100
# Clicks are per click, ad impression is the appearance of an ad (clicked or not)
ctr_id_box <- boxplot(ctr ~ companyId, data = train) 
ctr_filt <- train %>%  select(companyId, ctr) %>%  filter(ctr > 0)
ctr_filt_box <- boxplot(ctr ~ companyId, data = ctr_filt) # many 0 values for ID 43
ctr_zero <- train %>%  select(companyId, ctr) %>% group_by(companyId) %>% filter(ctr == 0) %>%  summarise(ZeroVal = n())
ctr_more <- train %>%  select(companyId, ctr) %>%  group_by(companyId) %>% summarise(Total = n())
ctr_tbl <- cbind(ctr_zero, ctr_more[2]) %>%  as.tbl %>%  mutate(Ratio = ZeroVal / Total,
                                                                NonZero = Total - ZeroVal)
# RATIO OF ZERO CTR TO TOTAL 
ggplot(data = ctr_tbl,
       mapping = aes(y = Ratio, x = companyId, fill = companyId)) +
  geom_bar(stat = 'identity', position = 'dodge') + 
  labs(title = 'Ratio of Zero Values against Total') # absolutely no difference to

# CPC against CTR
plot(cpc ~ ctr, data = train,
     ylab = 'Cost Per Click',
     xlab = 'Click Through Rate')
# CTR decreases to nothing, the less appealing the ad, the more likely the the advertiser is charged higher for the less appealing ad
```
```{r}
# Bivariate analysis of partitioned data with respect to Target Feature y
# CTR Summary
ctrzero_y <- train %>% select(companyId, ctr, y) %>%  filter(ctr == 0)
summary(ctrzero_y)
ctrmore_y <- train %>% select(companyId, ctr, y) %>%  filter(ctr > 0)
summary(ctrmore_y)
# all metrics (with exception of Max value) increase for y when looking at higher click through rates
# CPC Summary
cpczero_y <- train %>% select(companyId, cpc, y) %>%  filter(cpc == 0)
summary(cpczero_y)
cpcmore_y <- train %>%  select(companyId, cpc, y) %>% filter(cpc > 0)
summary(cpcmore_y)
# same results to CTR summary with rescpt to target feature y
```

# Statistical Investigation

## Linear Correlations

After examining the distributions of the descriptive features, we have decided to use Spearman's calculation for correlation rather than Pearson as many of the features appear to not follow a normal distribution.
```{r}


#Calculate correlation between numeric variables:
train_num <- train %>% select(price1, price2, price3, ad_area, ad_ratio, requests, impression, cpc, ctr, viewability, ratio1, ratio2, ratio3, ratio4, ratio5)
res <- cor(train_num, method = "spearman")

round(res, 2)

corrplot(res, order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

Notable Correlations:  

* Very strong positive correlation between Impression and Request of 0.99. These features are more or less redundant so only one will be included in the machine learning modelling. 

* Very strong positive correlation between Price 1, Price 2 and Price 3. Additional analysis conducted below.

* Very strong positive correlation between CPC and Impression of 0.88.

* Very strong positive correlation between CTR and Viewability of 0.88. 

* Very strong positive correlation between Ratio 4 and Ratio 5 of 0.83. Additional analysis conducted below. 



```{r}
#Focus on correlation between price variables:
train_price <- train %>% select(price1,price2,price3)
res_price <- cor(train_price, method = "spearman")
round(res_price,2)
```
Findings:  

* Correlation between Price 1 and Price 2: 0.97 

* Correlation between Price 2 and Price 3: 0.99

* Correlation between Price 3 and Price 1: 0.96

The strong positive correlation between the price measures indicates they all provide relatively similar information with respect to predicting the dependent variable y. Therefore, it is likely we only need to use price variable for training the algorithm as correlation is very high - potentially the average price calculated before. 

```{r}
#Focus on the correlation between ratio variables:
train_ratio <- train %>% select(ratio1, ratio2, ratio3, ratio4, ratio5)
res_ratio <- cor(train_ratio, method = "spearman")
round(res_ratio,2)
corrplot(res_ratio, order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
Findings:

* Correlation between Ratio 4 and Ratio 5 is 0.83

* Correlation between Ratio 1 and Ratio 2 is 0.72

* Correlation between Ratio 1 and Ratio 3 is 0.66

* Correlation between Ratio 2 and Ratio 3 is 0.62

* Correlation between Ratio 5 and Ratio 1 is 0.45

* All other ratio features have weaker correlation

The strong positive correlation between Ratio 4 and Ratio 5 raises the question as to whether both metrics are useful to conclude in the machine learning modelling. 

The rest of the ratios exhibit a sufficiently weak correlation that we would want to initally include them in modelling. 


```{r}
#Focus on the advertisement metrics data
train_metrics <- train %>% select(ad_area, ad_ratio, requests, impression, cpc, ctr, viewability)
res_metrics <- cor(train_metrics, method = "spearman")
round(res_metrics, 2)
corrplot(res_metrics, order = "hclust", 
         tl.col = "black", tl.srt = 45)

```
Findings:  

* Correlation between Impression and Requests is 0.99

* Correlation between Viewability and CTR is 0.88

* Correlation between CPC and Requests is 0.88

* Correlation between CPC and Impression is 0.88

* All other metrics have weak correlation

Given Impression and Requests are almost completely correlated, it is highly unlikely that we would want to include both features in our machine learning model.

```{r}
res_y <- cor(train$y,train_num, method = "spearman")
kable(round(res_y, 3))
```
None of the descriptive features are highly correlated with the target feature y. 

# Summary

The data set chosen for machine learning training is an advertising metrics data set provided by Google. First, we inspected the descriptive features and target feature in isolation. We noted that many of the descriptive features were zero for the majority of instances. We used a logarithmic transformation on the target feature (y) to improve the normality of its distribution. We then inspected the descriptive features with respect to the target feature y. We then took a closer look at the Price features and Ratio features, as well as two of the advertising metrics, Cost Per Click and Click Through Rate, in order to disaggregate instances with zero values and those with non-zero values. Finally, we calculated and inspected the correlation between the descriptive features and identified features that are likely to be redundant, and therefore of limited use to machine learning algorithm training.  
